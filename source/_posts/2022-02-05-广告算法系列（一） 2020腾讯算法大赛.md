---
title: 广告算法系列（一） 2020腾讯算法大赛
date: 2022-02-05 12:50:31
tags:
categories: 广告算法系列
---

特征工程

<!-- more -->

- [1. 知识总结](#1-知识总结)
    - [1.1. kaggle编码categorical feature总结](#11-kaggle编码categorical-feature总结)
    - [1.2. Mean (likelihood) encodings](#12-mean-likelihood-encodings)
- [2. 参赛团队 资料学习](#2-参赛团队-资料学习)
    - [2.1. 小太阳队](#21-小太阳队)
    - [2.2. 玉古路](#22-玉古路)
    - [2.3. 挥霍的人生](#23-挥霍的人生)
    - [2.4. xx](#24-xx)
    - [2.5. xx](#25-xx)
    - [2.6. xx](#26-xx)
    - [2.7. xx](#27-xx)
    - [2.8. xx](#28-xx)
    - [2.9. xx](#29-xx)
- [3. 代码资料](#3-代码资料)


地址： [【TAAC2020】2020腾讯广告算法大赛决赛 答辩（上）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1oK411J7iv?spm_id_from=333.999.0.0)

1. kaggle master
2. kdd2020
3. lightgbm


# 1. 知识总结

## 1.1. kaggle编码categorical feature总结
[kaggle编码categorical feature总结 - 知乎](https://zhuanlan.zhihu.com/p/40231966)

1. [kaggle编码categorical feature总结 - 知乎](https://zhuanlan.zhihu.com/p/40231966)
2. [Target Leakage | DataRobot Artificial Intelligence Wiki](https://www.datarobot.com/wiki/target-leakage/)
    1. “Any other feature whose value would not actually be available in practice at the time you’d want to use the model to make a prediction is a feature that can introduce leakage to your model.” – Data Skeptic
3. Leakage的解释 
    1. If you'd like to make a good prediction, your best bet is to invent a time machine, visit the future, observe the value, and return to the past. For those without access to time travel technology, we need to avoid including information about the future in our training data when building machine learning models. Similarily, if any other feature whose value would not actually be available in practice at the time you'd want to use the model to make a prediction, is a feature that can introduce leakage to your model.  from  [[MINI] Leakage](https://dataskeptic.com/blog/episodes/2016/leakage)

## 1.2. Mean (likelihood) encodings 

1. [Mean (likelihood) encodings: a comprehensive study | Kaggle](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study)
2. [Likelihood encoding of categorical features | Kaggle](https://www.kaggle.com/tnarik/likelihood-encoding-of-categorical-features/notebook)
3. [category_encoders/category_encoders at master · scikit-learn-contrib/category_encoders](https://github.com/scikit-learn-contrib/category_encoders/tree/master/category_encoders)
4. [On Grouping for Maximum Homogeneity](https://www.researchgate.net/publication/242580910_On_Grouping_for_Maximum_Homogeneity)
5. [Transforming categorical features to numerical features - How training is performed | CatBoost](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic)
6. [K-Fold Target Encoding. One-hot-encoding, label-encoding… | by Pourya | Medium](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b)
7. [Regularization and Cross-Validation — How to choose the penalty value (lambda) | by Swapnil Kangralkar | Analytics Vidhya | Medium](https://medium.com/analytics-vidhya/regularization-and-cross-validation-how-to-choose-the-penalty-value-lambda-1217fa4351e5)
8. [特征工程 - 知乎](https://www.zhihu.com/column/c_1114217169679921152)
9. [马东什么 - 知乎](https://www.zhihu.com/people/he-he-he-he-77-19-21/columns)
10. [target encoding - 知乎](https://zhuanlan.zhihu.com/p/136174936)
11. [CatBoost算法梳理 - 别再闹了 - 博客园](https://www.cnblogs.com/jiading/articles/12902632.html)
12. [The_Road_of_Data_Scientist/Boosting5.2 Catboost(生硬翻译).md at 10c5f8ef04c4938a29f945cea863ed5ba5f3c8c0 · DaiDuncan/The_Road_of_Data_Scientist](https://github.com/DaiDuncan/The_Road_of_Data_Scientist/blob/10c5f8ef04c4938a29f945cea863ed5ba5f3c8c0/310-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2%E7%BA%BF%20%E4%B8%93%E6%A0%8F(%E7%90%86%E8%AE%BA)/%E4%BD%93%E7%B3%BB%20ScikitLearn/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95(%E5%8A%A0%E7%B2%BE)/Boosting5.2%20Catboost(%E7%94%9F%E7%A1%AC%E7%BF%BB%E8%AF%91).md)
13. https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf
14. [catboost完全指南 - 知乎](https://zhuanlan.zhihu.com/p/102570430)


# 2. 参赛团队 资料学习

## 2.1. 小太阳队

1. tfidf 将用户点击过的广告组成序列计算tfidf特征。 参赛者表示该特征不是特别有效。
2. BiLSTM
    1. [（五）通俗易懂理解——BiLSTM - 知乎](https://zhuanlan.zhihu.com/p/40119926)
    2. [深度学习之GRU网络 - 微笑sun - 博客园](https://www.cnblogs.com/jiangxinyang/p/9376021.html)
3. transformer
    1. [None作为ndarray或tensor的索引_pyxiea-CSDN博客](https://blog.csdn.net/xpy870663266/article/details/105361313)
    2. [tf.einsum—爱因斯坦求和约定_欢迎来到打不死的小强的专栏-CSDN博客_爱因斯坦求和约定](https://blog.csdn.net/zzq060143/article/details/89107567)
    3. [Transformer-Text-Emotion-Classification/train.py at master · mengzhu0308/Transformer-Text-Emotion-Classification](https://github.com/mengzhu0308/Transformer-Text-Emotion-Classification/blob/master/codes/train.py)

## 2.2. 玉古路



1. category aware attention ?
2. 如何处理多层次 看不懂


## 2.3. 挥霍的人生

1. graph feature : DeepWalk/ProNE embedding, Deep Graph InfoMax
    1. [(35条消息) 网络表示学习（ProNE-2019IJCAI ）_我黑切呢**的博客-CSDN博客](https://blog.csdn.net/qq_43390809/article/details/107546823)
    2. [DEEP GRAPH INFOMAX 阅读笔记 - 知乎](https://zhuanlan.zhihu.com/p/58682802)
    3. [无监督Graph Embedding神器DGI，你学会了吗？ - 知乎](https://zhuanlan.zhihu.com/p/396341298)
    4. [(35条消息) GNN在文本分类上的应用 | (1) TextGCN_sdu_hao的博客-CSDN博客_gnn 文本分类](https://blog.csdn.net/sdu_hao/article/details/104202195)
2. 不同的模型需要考虑其训练性能

## 2.4. xx

1. 混合标签的思路挺好的，相当于是 预先用target信息指导 embedding训练。 这个感觉和target encode方式神似，再思考一下背后的意义。


## 2.5. xx

page rank、hits 图特征


序列shuffle 数据增强


## 2.6. xx
1. [文本匹配模型TextMatching - 简书](https://www.jianshu.com/p/dd5b27c7715b)
2. [文本匹配模型算法—推荐 - 知乎](https://zhuanlan.zhihu.com/p/372342136)
3. [Deep text matching--盘点11个文本匹配模型 - 知乎](https://zhuanlan.zhihu.com/p/144842132)
4. [文本匹配与ESIM模型详解_jesseyule的博客-CSDN博客_esim模型](https://blog.csdn.net/jesseyule/article/details/100579295)
5. [text matching(文本匹配) 相关资料总结_weixin_30316097的博客-CSDN博客](https://blog.csdn.net/weixin_30316097/article/details/99908000)

## 2.7. xx

1. [(35条消息) 【Embedding】EGES：阿里在图嵌入领域中的探索_阿泽的学习笔记-CSDN博客](https://blog.csdn.net/qq_27075943/article/details/106244434)
2. [ELMo原理解析及简单上手使用 - 知乎](https://zhuanlan.zhihu.com/p/51679783)
3. [学习笔记3——PU learning - 知乎](https://zhuanlan.zhihu.com/p/82556263)
4. [理解 YouTube Net 算法 - 知乎](https://zhuanlan.zhihu.com/p/109967716)
5. [香侬读 | 怎样在小数据集下学习OOV词向量？ - 知乎](https://zhuanlan.zhihu.com/p/72312668)
6. [label smoothing(标签平滑)学习笔记 - 知乎](https://zhuanlan.zhihu.com/p/116466239)

## 2.8. xx 
1. 信息孤岛 信息空岛


## 2.9. xx

[(35条消息) 掩码语言模型(Masked Language Model)mlm_kyle1314608的博客-CSDN博客_掩码语言模型](https://blog.csdn.net/kyle1314608/article/details/106030413)

Fusion layer

# 3. 代码资料

1. [2020腾讯广告算法大赛初赛rank6，复赛rank11方案简单分享及代码开源_wujiekd的博客-CSDN博客](https://blog.csdn.net/weixin_43999137/article/details/107657517)
2. [2020腾讯广告算法大赛——算法小白的复盘_诡途的博客-CSDN博客_腾讯广告算法大赛](https://blog.csdn.net/qq_35866846/article/details/107335288)
3. [2020腾讯广告算法大赛方案分享（冠军） - 知乎](https://zhuanlan.zhihu.com/p/166710532)
4. [guoday/Tencent2020_Rank1st: The code for 2020 Tencent College Algorithm Contest, and the online result ranks 1st.](https://github.com/guoday/Tencent2020_Rank1st)
5. [冠军10w美金 ! ! ! 备战2021腾讯广告算法大赛最新指南 - 知乎](https://zhuanlan.zhihu.com/p/354030648)
6. [2020腾讯广告算法大赛方案分享（季军）](https://mp.weixin.qq.com/s/rkhwLsCKTIDzUkjVIEj3LQ)
7. [Search · 2020 腾讯广告算法大赛](https://github.com/search?q=2020+%E8%85%BE%E8%AE%AF%E5%B9%BF%E5%91%8A%E7%AE%97%E6%B3%95%E5%A4%A7%E8%B5%9B)