---
title: 广告算法系列（一） 2020腾讯算法大赛
date: 2022-02-05 12:50:31
tags:
categories: 广告算法系列
---

特征工程

<!-- more -->

- [1. 知识总结](#1-知识总结)
    - [1.1. kaggle编码categorical feature总结](#11-kaggle编码categorical-feature总结)
    - [1.2. Mean (likelihood) encodings](#12-mean-likelihood-encodings)
- [2. 参赛团队 资料学习](#2-参赛团队-资料学习)
    - [2.1. 小太阳队](#21-小太阳队)
- [3. 代码资料](#3-代码资料)


地址： [【TAAC2020】2020腾讯广告算法大赛决赛 答辩（上）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1oK411J7iv?spm_id_from=333.999.0.0)

1. kaggle master
2. kdd2020
3. lightgbm


# 1. 知识总结

## 1.1. kaggle编码categorical feature总结
[kaggle编码categorical feature总结 - 知乎](https://zhuanlan.zhihu.com/p/40231966)

1. [kaggle编码categorical feature总结 - 知乎](https://zhuanlan.zhihu.com/p/40231966)
2. [Target Leakage | DataRobot Artificial Intelligence Wiki](https://www.datarobot.com/wiki/target-leakage/)
    1. “Any other feature whose value would not actually be available in practice at the time you’d want to use the model to make a prediction is a feature that can introduce leakage to your model.” – Data Skeptic
3. Leakage的解释 If you'd like to make a good prediction, your best bet is to invent a time machine, visit the future, observe the value, and return to the past. For those without access to time travel technology, we need to avoid including information about the future in our training data when building machine learning models. Similarily, if any other feature whose value would not actually be available in practice at the time you'd want to use the model to make a prediction, is a feature that can introduce leakage to your model.  from  [[MINI] Leakage](https://dataskeptic.com/blog/episodes/2016/leakage)

## 1.2. Mean (likelihood) encodings 

1. [Mean (likelihood) encodings: a comprehensive study | Kaggle](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study)
2. [Likelihood encoding of categorical features | Kaggle](https://www.kaggle.com/tnarik/likelihood-encoding-of-categorical-features/notebook)
3. [category_encoders/category_encoders at master · scikit-learn-contrib/category_encoders](https://github.com/scikit-learn-contrib/category_encoders/tree/master/category_encoders)
4. [On Grouping for Maximum Homogeneity](https://www.researchgate.net/publication/242580910_On_Grouping_for_Maximum_Homogeneity)
5. [Transforming categorical features to numerical features - How training is performed | CatBoost](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic)
6. [K-Fold Target Encoding. One-hot-encoding, label-encoding… | by Pourya | Medium](https://medium.com/@pouryaayria/k-fold-target-encoding-dfe9a594874b)
7. [Regularization and Cross-Validation — How to choose the penalty value (lambda) | by Swapnil Kangralkar | Analytics Vidhya | Medium](https://medium.com/analytics-vidhya/regularization-and-cross-validation-how-to-choose-the-penalty-value-lambda-1217fa4351e5)
8. [特征工程 - 知乎](https://www.zhihu.com/column/c_1114217169679921152)
9. [马东什么 - 知乎](https://www.zhihu.com/people/he-he-he-he-77-19-21/columns)
10. [target encoding - 知乎](https://zhuanlan.zhihu.com/p/136174936)
11. [CatBoost算法梳理 - 别再闹了 - 博客园](https://www.cnblogs.com/jiading/articles/12902632.html)
12. [The_Road_of_Data_Scientist/Boosting5.2 Catboost(生硬翻译).md at 10c5f8ef04c4938a29f945cea863ed5ba5f3c8c0 · DaiDuncan/The_Road_of_Data_Scientist](https://github.com/DaiDuncan/The_Road_of_Data_Scientist/blob/10c5f8ef04c4938a29f945cea863ed5ba5f3c8c0/310-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2%E7%BA%BF%20%E4%B8%93%E6%A0%8F(%E7%90%86%E8%AE%BA)/%E4%BD%93%E7%B3%BB%20ScikitLearn/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95(%E5%8A%A0%E7%B2%BE)/Boosting5.2%20Catboost(%E7%94%9F%E7%A1%AC%E7%BF%BB%E8%AF%91).md)
13. https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf
14. [catboost完全指南 - 知乎](https://zhuanlan.zhihu.com/p/102570430)


# 2. 参赛团队 资料学习

## 2.1. 小太阳队

1. tfidf 将用户点击过的广告组成序列计算tfidf特征。 参赛者表示该特征不是特别有效。
2. BiLSTM
    1. [（五）通俗易懂理解——BiLSTM - 知乎](https://zhuanlan.zhihu.com/p/40119926)
    2. [深度学习之GRU网络 - 微笑sun - 博客园](https://www.cnblogs.com/jiangxinyang/p/9376021.html)
3. transformer
    1. [None作为ndarray或tensor的索引_pyxiea-CSDN博客](https://blog.csdn.net/xpy870663266/article/details/105361313)
    2. [tf.einsum—爱因斯坦求和约定_欢迎来到打不死的小强的专栏-CSDN博客_爱因斯坦求和约定](https://blog.csdn.net/zzq060143/article/details/89107567)
    3. [Transformer-Text-Emotion-Classification/train.py at master · mengzhu0308/Transformer-Text-Emotion-Classification](https://github.com/mengzhu0308/Transformer-Text-Emotion-Classification/blob/master/codes/train.py)


# 3. 代码资料

1. [2020腾讯广告算法大赛初赛rank6，复赛rank11方案简单分享及代码开源_wujiekd的博客-CSDN博客](https://blog.csdn.net/weixin_43999137/article/details/107657517)
2. [2020腾讯广告算法大赛——算法小白的复盘_诡途的博客-CSDN博客_腾讯广告算法大赛](https://blog.csdn.net/qq_35866846/article/details/107335288)
3. [2020腾讯广告算法大赛方案分享（冠军） - 知乎](https://zhuanlan.zhihu.com/p/166710532)
4. [guoday/Tencent2020_Rank1st: The code for 2020 Tencent College Algorithm Contest, and the online result ranks 1st.](https://github.com/guoday/Tencent2020_Rank1st)
5. [冠军10w美金 ! ! ! 备战2021腾讯广告算法大赛最新指南 - 知乎](https://zhuanlan.zhihu.com/p/354030648)
6. [2020腾讯广告算法大赛方案分享（季军）](https://mp.weixin.qq.com/s/rkhwLsCKTIDzUkjVIEj3LQ)
7. [Search · 2020 腾讯广告算法大赛](https://github.com/search?q=2020+%E8%85%BE%E8%AE%AF%E5%B9%BF%E5%91%8A%E7%AE%97%E6%B3%95%E5%A4%A7%E8%B5%9B)